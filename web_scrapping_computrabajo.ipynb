{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración de la URL que queremos scrappear y los encabezados HTTP\n",
    "\n",
    "BASE_URL = \"https://co.computrabajo.com/trabajo-de-analista-de-datos\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "respuesta = requests.get(BASE_URL, headers=HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una lista vacia donde vamos a añadir las ofertas de trabajo\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número máximo de páginas que queremos scrappear\n",
    "max_paginas = int(input(\"Cuántas páginas deseas scrappear?: \"))\n",
    "\n",
    "pagina = 1\n",
    "\n",
    "while pagina <= max_paginas:\n",
    "    # Construimos la url\n",
    "    url_pagina = BASE_URL if pagina == 1 else f\"{BASE_URL}?p={pagina}\"\n",
    "    print(f\"Procesando página {pagina} - {url_pagina}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        respuesta = requests.get(url_pagina, headers=HEADERS)\n",
    "        respuesta.raise_for_status()  # controlamos la respuesta HTTP en caso de que algun error\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al hacer la solicitud: {e}\")\n",
    "        break\n",
    "\n",
    "    if respuesta.status_code == 200:\n",
    "        soup = BeautifulSoup(respuesta.text, \"html.parser\")\n",
    "        \n",
    "        # Encontrar todas las ofertas de trabajo\n",
    "        ofertas = soup.find_all(\"article\", class_=\"box_offer\")\n",
    "        \n",
    "        # si no hay mas ofertas, detenemos el bucle\n",
    "        if not ofertas:\n",
    "            print(\"No se encontraron más ofertas\")\n",
    "            break\n",
    "\n",
    "        for oferta in ofertas:\n",
    "            titulo_elemento = oferta.find(\"a\", class_=\"js-o-link fc_base\")\n",
    "            empresa_elemento = oferta.find(\"a\", class_=\"fc_base t_ellipsis\")\n",
    "            ubicacion_elemento = oferta.find(\"p\", class_=\"fs16 fc_base mt5\")  \n",
    "            salario_elemento = oferta.find(\"span\", class_=\"dIB mr10\")\n",
    "\n",
    "            # Extraer texto con None\n",
    "            titulo = titulo_elemento.get_text(strip=True) if titulo_elemento else \"No disponible\"\n",
    "            enlace = f\"https://co.computrabajo.com{titulo_elemento['href']}\" if titulo_elemento else \"No disponible\"\n",
    "            empresa = empresa_elemento.get_text(strip=True) if empresa_elemento else \"No disponible\"\n",
    "            ubicacion = ubicacion_elemento.get_text(strip=True) if ubicacion_elemento else \"No disponible\"\n",
    "            salario = salario_elemento.get_text(strip=True) if salario_elemento else \"No disponible\"\n",
    "\n",
    "            # Agregamos los datos a la lista vacia\n",
    "            data.append({\n",
    "                \"Título\": titulo,\n",
    "                \"Empresa\": empresa,\n",
    "                \"Ubicación\": ubicacion,\n",
    "                \"Salario\": salario,\n",
    "                \"Enlace\": enlace\n",
    "            })\n",
    "\n",
    "        # Pausa aleatoria para evitar bloqueos, entre 1 y 3 segundos, puede ser mas para que estemos mas seguros\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        # Pasamos a la siguiente pagina\n",
    "        pagina += 1\n",
    "    else:\n",
    "        print(f\"Error al acceder a la página, estado: {respuesta.status_code}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data) # creamos un Dataframe con la data que obtuvimos\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Función para extraer la descripción de una oferta individual\n",
    "def obtener_descripcion(enlace):\n",
    "    try:\n",
    "        respuesta = requests.get(enlace, headers=HEADERS)\n",
    "        respuesta.raise_for_status()\n",
    "        soup = BeautifulSoup(respuesta.text, \"html.parser\")\n",
    "        descripcion_elemento = soup.find(\"p\", class_=\"mbB\")\n",
    "        \n",
    "        return descripcion_elemento.get_text(strip=True) if descripcion_elemento else \"No disponible\"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al acceder al enlace {enlace}: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "\n",
    "# Obtenemos las descripciones\n",
    "descripciones = []\n",
    "for enlace in df[\"Enlace\"]:\n",
    "    descripcion = obtener_descripcion(enlace)\n",
    "    descripciones.append(descripcion)\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "\n",
    "# luego creamos una nueva columna a la cual le vamos a añadir las descripciones\n",
    "df[\"Descripción\"] = descripciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Ofertas_Computrabajo.csv\",index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
